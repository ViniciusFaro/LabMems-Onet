{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping functions\n",
    "\n",
    "def to_mlp(x):\n",
    "    B, C, H, W = x.size()\n",
    "    return x.reshape(B*H*W, C)\n",
    "\n",
    "def to_chw(x, dims):\n",
    "    B, C, H, W = dims\n",
    "    return x.reshape(B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 256, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(size=[32, 256, 64,64])\n",
    "\n",
    "x.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap patch embedding\n",
    "\n",
    "class OverlapPatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, embbed_dim):\n",
    "        super(OverlapPatchEmbedding, self).__init__()\n",
    "\n",
    "        # input: [B, C, H, W]\n",
    "        self.c_in = in_channels\n",
    "        self.c_out = embbed_dim\n",
    "\n",
    "        self.encode = nn.Conv2d(in_channels=self.c_in, out_channels=self.c_out, kernel_size=7, stride=4, padding=3)\n",
    "\n",
    "        self.activation = nn.GELU() # added after, maybe can lead to more learning capacity\n",
    "\n",
    "        self.decode = nn.ConvTranspose2d(in_channels=self.c_out, out_channels=self.c_out, kernel_size=6, stride=4, padding=1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.activation(self.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention - transformer block\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedd_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.query_weight = nn.Linear(in_features=embedd_dim, out_features=embedd_dim)\n",
    "        self.key_weight = nn.Linear(in_features=embedd_dim, out_features=embedd_dim)\n",
    "        self.value_weight = nn.Linear(in_features=embedd_dim, out_features=embedd_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        x = x.reshape(B, H*W, C)\n",
    "\n",
    "        # query, key and value tensors\n",
    "        q = self.query_weight(x)\n",
    "        k = self.key_weight(x)\n",
    "        v = self.value_weight(x)\n",
    "\n",
    "        scaled = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "        scaled = F.softmax(scaled / math.sqrt(C), dim=-1)\n",
    "\n",
    "        attention = torch.bmm(scaled, v)\n",
    "\n",
    "        return attention.reshape(B, C, H, W)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIX_FFN(nn.Module):\n",
    "    def __init__(self, embedd_dim):\n",
    "        super(MIX_FFN, self).__init__()\n",
    "\n",
    "        self.mlp1 = nn.Linear(embedd_dim, 2 * embedd_dim)\n",
    "        \n",
    "        self.conv = nn.Conv2d(2 * embedd_dim, 2 * embedd_dim, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self.mlp2 = nn.Linear(2 * embedd_dim, embedd_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H , W = x.size()\n",
    "        x_ = x\n",
    "\n",
    "        # mlp1\n",
    "        x = to_mlp(x)\n",
    "        x = self.mlp1(x)\n",
    "\n",
    "        # conv\n",
    "        x = to_chw(x, [B, 2*C, H, W])\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # gelu\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # mlp2\n",
    "        x = to_mlp(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = to_chw(x, [B, C, H, W])\n",
    "\n",
    "        return x + x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 22, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = MIX_FFN(32)\n",
    "x = torch.randn(16, 32, 22,16)\n",
    "ffn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchMerging(nn.Module):\n",
    "    def __init__(self, embedd_dim):\n",
    "        super(OverlapPatchMerging, self).__init__()\n",
    "        # divide as dimensões espaciais por 2]\n",
    "        # no artigo original, divide-se as dimensões espaciais por 4, mas por questão do formato dos dados do microCT, vamos dividir apenas por 2\n",
    "        self.conv = nn.Conv2d(embedd_dim, 2*embedd_dim, kernel_size=7, stride=2, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 11, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opm = OverlapPatchMerging(32)\n",
    "opm(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerChain(nn.Module):\n",
    "    def __init__(self, embedd_dim):\n",
    "        super(TransformerChain, self).__init__()\n",
    "\n",
    "        # 4 self-att\n",
    "        # 4 mix-ffn\n",
    "\n",
    "        self.att1 = SelfAttention(embedd_dim)\n",
    "        self.ffn1 = MIX_FFN(embedd_dim)\n",
    "\n",
    "        self.att2 = SelfAttention(embedd_dim)\n",
    "        self.ffn2 = MIX_FFN(embedd_dim)\n",
    "\n",
    "        self.att3 = SelfAttention(embedd_dim)\n",
    "        self.ffn3 = MIX_FFN(embedd_dim)\n",
    "\n",
    "        self.att4 = SelfAttention(embedd_dim)\n",
    "        self.ffn4 = MIX_FFN(embedd_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn4(self.att4(self.ffn3(self.att3(self.ffn2(self.att2(self.ffn1(self.att1(x))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 10, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TransformerChain(2)\n",
    "x = torch.randn(16, 2, 10, 5)\n",
    "t(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedd_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # encoder 1\n",
    "        self.t1 = TransformerChain(embedd_dim)\n",
    "        self.opm1 = OverlapPatchMerging(embedd_dim)\n",
    "\n",
    "        # encoder 2\n",
    "        self.t2 = TransformerChain(2 * embedd_dim)\n",
    "        self.opm2 = OverlapPatchMerging(2 * embedd_dim)\n",
    "\n",
    "        # encoder 3\n",
    "        self.t3 = TransformerChain(4 * embedd_dim)\n",
    "        self.opm3 = OverlapPatchMerging(4 * embedd_dim)\n",
    "\n",
    "        # encoder 4\n",
    "        self.t4 = TransformerChain(8 * embedd_dim)\n",
    "        self.opm4 = OverlapPatchMerging(8 * embedd_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.opm4(self.t4(self.opm3(self.t3(self.opm2(self.t2(self.opm1(self.t1(x))))))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(2)\n",
    "x = torch.randn(1, 2, 16, 16)\n",
    "\n",
    "enc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6576668672 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, embedd_dim)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# encoder 4\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt4 \u001b[38;5;241m=\u001b[39m TransformerChain(\u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m embedd_dim)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopm4 \u001b[38;5;241m=\u001b[39m \u001b[43mOverlapPatchMerging\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedd_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mOverlapPatchMerging.__init__\u001b[1;34m(self, embedd_dim)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(OverlapPatchMerging, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# divide as dimensões espaciais por 2]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# no artigo original, divide-se as dimensões espaciais por 4, mas por questão do formato dos dados do microCT, vamos dividir apenas por 2\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedd_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43membedd_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    445\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    446\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:134\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m    132\u001b[0m         (in_channels, out_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m groups, \u001b[38;5;241m*\u001b[39mkernel_size), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_channels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6576668672 bytes."
     ]
    }
   ],
   "source": [
    "encoder = Encoder(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
